{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Autoencoder in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will answer some common questions about autoencoders, and we will cover code examples of the following models:\n",
    "\n",
    "    - a simple autoencoder based on a fully-connected layer\n",
    "    - a sparse autoencoder (not currently working)\n",
    "    - a deep fully-connected autoencoder\n",
    "    - a deep convolutional autoencoder (also available in part 1)\n",
    "    \n",
    "    \n",
    "**Note: all code examples have been updated to the Keras 2.0 API on March 14, 2017. You will need Keras version 2.0.0 or higher to run them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Environment\n",
    "\n",
    "This tutorial assumes you have a Python SciPy environment installed. You can use either Python 2 or 3 with this tutorial.\n",
    "\n",
    "You must have Keras (2.0 or higher) installed with either the TensorFlow or Theano backend.\n",
    "\n",
    "The tutorial also assumes you have scikit-learn, Pandas, NumPy, and Matplotlib installed.\n",
    "\n",
    "If you need help with your environment, [this](https://machinelearningmastery.com/setup-python-environment-machine-learning-deep-learning-anaconda/) link is here to rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are autoencoders?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= 'https://blog.keras.io/img/ae/autoencoder_schema.jpg'/>\n",
    "\n",
    "\"Autoencoding\" is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) learned automatically from examples rather than engineered by a human. Additionally, in almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.\n",
    "\n",
    "    1) Autoencoders are data-specific, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about \"sound\" in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.\n",
    "    \n",
    "    2) Autoencoders are lossy, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.\n",
    "    \n",
    "    3) Autoencoders are learned automatically from data examples, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require any new engineering, just appropriate training data.\n",
    "\n",
    "To build an autoencoder, you need three things: \n",
    "    - an encoding function, \n",
    "    - a decoding function, and \n",
    "    - a distance function between the amount of information loss between the compressed representation of your data and the decompressed representation (i.e. a \"loss\" function). \n",
    "\n",
    "The encoder and decoder will be chosen to be parametric functions (typically neural networks), and to be differentiable with respect to the distance function, so the parameters of the encoding/decoding functions can be optimize to minimize the reconstruction loss, using Stochastic Gradient Descent. It's simple! And you don't even need to understand any of these words to start using autoencoders in practice.\n",
    "\n",
    "Note that even Autoencoding is a data compression algorithm, autoencoders are *not* good at data compression, so ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are autoencoders good for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today two interesting practical applications of autoencoders are data denoising (which we feature later in this post), and dimensionality reduction for data visualization. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.\n",
    "\n",
    "For 2D visualization specifically, [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) (pronounced \"tee-snee\") is probably the best algorithm around, but it typically requires relatively low-dimensional data. So a good strategy for visualizing similarity relationships in high-dimensional data is to start by using an autoencoder to compress your data into a low-dimensional space (e.g. 32 dimensional), then use t-SNE for mapping the compressed data to a 2D plane. Note that a nice parametric implementation of t-SNE in Keras was developed by Kyle McDonald and is available on [Github](https://github.com/kylemcdonald/Parametric-t-SNE/blob/master/Parametric%20t-SNE%20(Keras).ipynb). Otherwise [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) also has a simple and practical implementation.\n",
    "\n",
    "Autoencoders have long been thought to be a potential avenue for solving the problem of **unsupervised learning**, i.e. the learning of useful representations without the need for labels. Then again, autoencoders are not a **true** unsupervised learning technique (which would imply a different learning process altogether), they are a *self-supervised* technique, *a specific instance of supervised learning where the targets are generated from the input data*. In order to get self-supervised models to learn interesting features, you have to come up with an interesting synthetic target and loss function, and that's where problems arise: merely learning to reconstruct your input in minute detail might not be the right choice here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build the simplest possible autoencoder\n",
    "\n",
    "We'll start simple, with a single fully-connected neural layer as encoder and as decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a separate encoder and a decoder for illustration purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our autoencoder to reconstruct MNIST digits.\n",
    "\n",
    "First, we'll configure our model to use a per-pixel **binary crossentropy loss**, and the **Adadelta** optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 784)               25872     \n",
      "=================================================================\n",
      "Total params: 50,992\n",
      "Trainable params: 50,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For your information \n",
    "Cross-entropy loss, or *log loss*, measures the performance of a classification model whose output is a probability value between 0 and 1. The predictive results from most binary classification model are **not** 0/1, but a probability between [0,1].\n",
    "\n",
    "Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A **perfect** model would have a log loss of 0.\n",
    "\n",
    "In other words, a classifier is good when log loss is low (close to 0). In this case, log loss refer to in each pixel of the image, is it the same (1) or different (0) comparing between the original and decoded images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our input data. We're using MNIST digits, and we're discarding the labels (since we're only interested in encoding/decoding the input images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784. The value between 0 and 1 refers to the grayscale value of the pixel in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Q: Use the **context of our classifcaition problem**, please explain what the two tuple above means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: (The number of examples, the dimensionality of the vector/ if the image is flattened, how long the vector is, or the number of pixels that was present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our autoencoder for 50 epochs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For your information\n",
    "\n",
    "In machine-learning parlance, an epoch is a **complete** pass through a given dataset. That is, by the end of one epoch, your neural network – be it a restricted Boltzmann machine, convolutional net or deep-belief network – will have been exposed to *every record to example within the dataset once*. Not to be confused with an iteration, which is simply **one** update of the neural net model’s parameters. **Many iterations can occur before an epoch is over**. Epoch and iteration are only synonymous if you update your parameters once for each pass through the whole dataset; if you update using mini-batches, they mean different things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/tljh/user/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.3613 - val_loss: 0.2716\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.2649 - val_loss: 0.2544\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.2433 - val_loss: 0.2301\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.2221 - val_loss: 0.2121\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.2069 - val_loss: 0.1995\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1962 - val_loss: 0.1903\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1881 - val_loss: 0.1830\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1814 - val_loss: 0.1769\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1755 - val_loss: 0.1715\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1704 - val_loss: 0.1667\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1659 - val_loss: 0.1624\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1618 - val_loss: 0.1584\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1580 - val_loss: 0.1548\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1545 - val_loss: 0.1515\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1513 - val_loss: 0.1483\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1482 - val_loss: 0.1455\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1454 - val_loss: 0.1426\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1427 - val_loss: 0.1399\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1402 - val_loss: 0.1375\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1378 - val_loss: 0.1352\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1356 - val_loss: 0.1329\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1334 - val_loss: 0.1308\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1314 - val_loss: 0.1287\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1294 - val_loss: 0.1269\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1275 - val_loss: 0.1250\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1257 - val_loss: 0.1232\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1240 - val_loss: 0.1215\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1224 - val_loss: 0.1200\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1209 - val_loss: 0.1186\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1194 - val_loss: 0.1171\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1181 - val_loss: 0.1157\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1168 - val_loss: 0.1145\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1156 - val_loss: 0.1134\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1145 - val_loss: 0.1123\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1135 - val_loss: 0.1113\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1125 - val_loss: 0.1104\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1117 - val_loss: 0.1096\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1109 - val_loss: 0.1088\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1101 - val_loss: 0.1081\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1094 - val_loss: 0.1074\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1088 - val_loss: 0.1069\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1082 - val_loss: 0.1063\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1077 - val_loss: 0.1058\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1072 - val_loss: 0.1053\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1068 - val_loss: 0.1049\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1063 - val_loss: 0.1045\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1060 - val_loss: 0.1041\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1056 - val_loss: 0.1038\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1053 - val_loss: 0.1034\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1049 - val_loss: 0.1031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2bab811630>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "Q: Please observe the training history, and answer whether the autoencoder is improved after training? And why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: (yes; val_loss is decreasing with each epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 50 epochs, the autoencoder seems to reach a stable train/test loss value of about 0.11. We can try to visualize the reconstructed inputs and the encoded representations. We will use Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we get. The top row is the original digits, and the bottom row is the reconstructed digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "Q: please discuss how the reconstructed images comparing to the original images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: (The orginal digits are 'sharper'/ have more 'noise', but the reconstructed images capture the essence of the original images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-on: how to improve the model?\n",
    "The quality of the autoencoder we trained, or of any basic autoencoder, is depending on two parameters:\n",
    "    - Encoding dimension (in code block [3])\n",
    "    - Number of Epochs (in code block [8])\n",
    "Try changing these two parameters, and rerun the code (do not forget to **restart the kernel**), to see how to optimize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve the performance of our autoencoder, we do not have to limit ourselves to a single layer as encoder or decoder, we could instead use a stack of layers, such as:\n",
    "\n",
    "**NOTE that you have to pair an encoder with a decoder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(128, activation='relu')(input_img) #begin compression\n",
    "encoded = Dense(64, activation='relu')(encoded) #more compression\n",
    "\n",
    "#compressed input\n",
    "encoded = Dense(32, activation='relu')(encoded) #compressed input\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded) #begin decompression\n",
    "decoded = Dense(128, activation='relu')(decoded) #more decompression\n",
    "decoded = Dense(784, activation='sigmoid')(decoded) #output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 10s 164us/step - loss: 0.3452 - val_loss: 0.2639\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.2583 - val_loss: 0.2531\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.2441 - val_loss: 0.2369\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.2299 - val_loss: 0.2232\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.2152 - val_loss: 0.2044\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.2007 - val_loss: 0.1960\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.1939 - val_loss: 0.1895\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.1860 - val_loss: 0.1819\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.1786 - val_loss: 0.1730\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1720 - val_loss: 0.1673\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1666 - val_loss: 0.1631\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1625 - val_loss: 0.1617\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.1591 - val_loss: 0.1555\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1559 - val_loss: 0.1543\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1532 - val_loss: 0.1518\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.1509 - val_loss: 0.1492\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1486 - val_loss: 0.1447\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.1465 - val_loss: 0.1439\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1440 - val_loss: 0.1419\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.1419 - val_loss: 0.1391\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1402 - val_loss: 0.1392\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1385 - val_loss: 0.1379\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1371 - val_loss: 0.1341\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.1357 - val_loss: 0.1345\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.1345 - val_loss: 0.1327\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.1330 - val_loss: 0.1319\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1319 - val_loss: 0.1311\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1308 - val_loss: 0.1288\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1296 - val_loss: 0.1278\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1286 - val_loss: 0.1273\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1278 - val_loss: 0.1258\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.1270 - val_loss: 0.1246\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1260 - val_loss: 0.1257\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1253 - val_loss: 0.1226\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1246 - val_loss: 0.1232\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1239 - val_loss: 0.1218\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1231 - val_loss: 0.1199\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.1226 - val_loss: 0.1216\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1221 - val_loss: 0.1203\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.1211 - val_loss: 0.1176\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1206 - val_loss: 0.1200\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.1200 - val_loss: 0.1181\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.1194 - val_loss: 0.1181\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1188 - val_loss: 0.1174\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.1182 - val_loss: 0.1182\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1178 - val_loss: 0.1162\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.1173 - val_loss: 0.1151\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1168 - val_loss: 0.1148\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1163 - val_loss: 0.1140\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1160 - val_loss: 0.1135\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1155 - val_loss: 0.1133\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.1151 - val_loss: 0.1139\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1147 - val_loss: 0.1137\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1143 - val_loss: 0.1122\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1138 - val_loss: 0.1134\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1135 - val_loss: 0.1128\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1130 - val_loss: 0.1107\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.1125 - val_loss: 0.1156\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.1122 - val_loss: 0.1115\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.1118 - val_loss: 0.1131\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.1114 - val_loss: 0.1096\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.1110 - val_loss: 0.1098\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.1107 - val_loss: 0.1095\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1103 - val_loss: 0.1095\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1099 - val_loss: 0.1104\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1095 - val_loss: 0.1090\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1092 - val_loss: 0.1071\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1088 - val_loss: 0.1082\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.1085 - val_loss: 0.1072\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1081 - val_loss: 0.1094\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1078 - val_loss: 0.1063\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1076 - val_loss: 0.1062\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1072 - val_loss: 0.1072\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1070 - val_loss: 0.1066\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1068 - val_loss: 0.1037\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1065 - val_loss: 0.1044\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1063 - val_loss: 0.1050\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.1060 - val_loss: 0.1057\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.1059 - val_loss: 0.1047\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.1055 - val_loss: 0.1059\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.1054 - val_loss: 0.1052\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.1052 - val_loss: 0.1025\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1049 - val_loss: 0.1044\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.1049 - val_loss: 0.1040\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1045 - val_loss: 0.1025\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1043 - val_loss: 0.1031\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.1042 - val_loss: 0.1026\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.1039 - val_loss: 0.1029\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1038 - val_loss: 0.1033\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1037 - val_loss: 0.1018\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1033 - val_loss: 0.1042\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1032 - val_loss: 0.1026\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1030 - val_loss: 0.1045\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.1029 - val_loss: 0.1036\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1028 - val_loss: 0.1032\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1025 - val_loss: 0.0997\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1023 - val_loss: 0.1025\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.1021 - val_loss: 0.1022\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.1020 - val_loss: 0.1014\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.1016 - val_loss: 0.1012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2bab563080>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe from the training history, in order to learn the weights in all the neurons of the encoding/decoding layes, we need more epochs (100 vs. 50) to get a slightly better loss. The models ends with a train loss of 0.1017 and test loss of 0.1011. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR TURN HERE\n",
    "Q: Comparing this to the results in the previous example, is it better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: (The images are not necessarily better.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional autoencoder [Demo-only]\n",
    "\n",
    "Since our inputs are images, it makes sense to use [convolutional neural networks (convnets)](https://www.safaribooksonline.com/library/view/deep-learning/9781491924570/ch04.html) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders -- they simply perform much better.\n",
    "\n",
    "Let's implement one. The encoder will consist in a stack of Conv2D and MaxPooling2D layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of Conv2D and UpSampling2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train it, we will use the original MNIST digits with shape (samples, 3, 28, 28), and we will just normalize pixel values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# simple early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for showing images\n",
    "def show_imgs(x_test, decoded_imgs=None, n=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i+1)\n",
    "        plt.imshow(x_test[i].reshape(28,28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        if decoded_imgs is not None:\n",
    "            ax = plt.subplot(2, n, i+ 1 +n)\n",
    "            plt.imshow(decoded_imgs[i].reshape(28,28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "print(\"input (upper row)\\ndecoded (bottom row)\")\n",
    "show_imgs(x_test, decoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
